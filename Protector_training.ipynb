{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f37c0-fbc5-4304-ab21-80fe1f169c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "tqdm.pandas()\n",
    "import datetime\n",
    "import argparse\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel\n",
    "from transformers.modeling_utils import top_k_top_p_filtering\n",
    "from torch import nn\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"The ValueHead class implements a head for GPT2 that returns a scalar for each output token.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.detach_head = False\n",
    "        self.summary_type = config.summary_type if hasattr(config, \"summary_type\") else \"last\"\n",
    "        if self.summary_type == \"attn\":\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        self.summary = Identity()\n",
    "        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n",
    "            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n",
    "                num_classes = config.num_labels\n",
    "            else:\n",
    "                num_classes = config.hidden_size\n",
    "            self.summary = nn.Linear(config.hidden_size, num_classes)  #768->1\n",
    "            \n",
    "        self.activation = Identity()  #this\n",
    "        if hasattr(config, \"summary_activation\") and config.summary_activation == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "            \n",
    "        self.first_dropout = Identity()  #0.1\n",
    "        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n",
    "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
    "            \n",
    "        self.last_dropout = Identity()\n",
    "        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n",
    "            self.last_dropout = nn.Dropout(config.summary_last_dropout)  \n",
    "            \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, hidden_states, cls_index=None):\n",
    "  \n",
    "        if self.detach_head:\n",
    "            output = hidden_states.detach()\n",
    "        else:\n",
    "            output = hidden_states\n",
    "        output = self.first_dropout(output)  #有，0.1\n",
    "        output = self.summary(output)        # 768->1\n",
    "        output = self.activation(output)     #无\n",
    "        output = self.last_dropout(output)   #无\n",
    "        return output\n",
    "\n",
    "\n",
    "class GPT2HeadWithValueModel(GPT2PreTrainedModel):\n",
    "    \"\"\"The GPT2HeadWithValueModel class implements a GPT2 language model with a secondary, scalar head.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # print(config)\n",
    "        # print('vocab_size shape :',config.vocab_size)\n",
    "        config.num_labels = 1\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.v_head = ValueHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def detach_value_head(self):\n",
    "        self.v_head.detach_head = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        mc_token_ids=None,\n",
    "        lm_labels=None,\n",
    "        mc_labels=None,\n",
    "    ):\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past=past,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        value = self.v_head(hidden_states).squeeze(-1)\n",
    "        outputs = (lm_logits,) + transformer_outputs[1:] + (value,)\n",
    "        return outputs\n",
    "\n",
    "def respond_to_batch(model , queries, txt_len=20, top_k=0, top_p=1.0):\n",
    "    \"\"\"Sample text from language model.\"\"\"\n",
    "    input_ids = queries\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    with torch.no_grad():  \n",
    "        for i in range(txt_len):\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs[0][:, -1, :]\n",
    "\n",
    "            next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "    return input_ids, outputs[2], outputs[0]\n",
    "\n",
    "def generate(model, total_nums,  txt_out_len):\n",
    "    #start 项审生成\n",
    "    bs = 64\n",
    "    wafData=pd.DataFrame()\n",
    "    wafData['content']=['0' for _ in range(300)]\n",
    "    wafData['tokens']=wafData['content'].progress_apply(lambda x: gpt2_tokenizer.encode(x, return_tensors=\"pt\").to(device)[0, :txt_in_len])\n",
    "    wafData['query'] = wafData['tokens'].progress_apply(lambda x: gpt2_tokenizer.decode(x))\n",
    "\n",
    "    #生成过程\n",
    "    valueList =torch.tensor([ ], device='cuda:0')  #store every payload's current value \n",
    "    responseList_original= torch.tensor([], device='cuda:0').int()  \n",
    "    probList =torch.tensor([ ], device='cuda:0')\n",
    "    while len(responseList_original)<total_nums:\n",
    "        torch.cuda.empty_cache()\n",
    "        df_batch = wafData.sample(bs)\n",
    "        query_tensors = torch.stack(df_batch['tokens'].tolist())\n",
    "        response_tensors, valuelist1, problist1 = respond_to_batch(model, query_tensors, txt_len=txt_out_len)\n",
    "        valueList= torch.cat((valueList, valuelist1), dim=0)  # total_nums * txt_out_len   记录每一项生成的current value\n",
    "        probList= torch.cat((probList, problist1), dim=0)     # total_nums * txt_out_len *vocabsize  记录生成过程中每一个选项的具体概率，未进行softmax\n",
    "        responseList_original= torch.cat((responseList_original, response_tensors), dim=0)  #total_nums * (txt_out_len+1)  记录模型生成的 token，未进行decoder\n",
    "    return  valueList, probList,responseList_original\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "tqdm.pandas()\n",
    "import datetime\n",
    "import argparse\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "\n",
    "# 读取数据\n",
    "# 初始化 GPT-2 模型和 tokenizer\n",
    "base_model_path = \"/data/shikangwei/gptrlt/givenmodel/pretrain_model_sql\"\n",
    "model = GPT2Model.from_pretrained(base_model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(base_model_path)\n",
    "config = GPT2Config.from_pretrained(base_model_path)\n",
    "\n",
    "class GPT2RewardPredictor(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2RewardPredictor, self).__init__()\n",
    "        self.gpt2 = GPT2Model(config)\n",
    "        self.reward_prediction_head = nn.Linear(config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Assuming each element in batch_inputs is a tensor\n",
    "        # Unpack the input tensors and make predictions for each element\n",
    "        predictions = [self.predict_single(input_tensor) for input_tensor in inputs]\n",
    "        # Combine the predictions into a single tensor\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "        return predictions\n",
    "\n",
    "    def predict_single(self, input_ids):\n",
    "        # Assuming input_ids is a tensor of shape (batch_size, sequence_length)\n",
    "        outputs = self.gpt2(input_ids)\n",
    "        last_hidden_states = outputs[0]\n",
    "        \n",
    "        # Assuming last_hidden_states is of shape (batch_size, sequence_length, hidden_size)\n",
    "        transformer_output = last_hidden_states\n",
    "\n",
    "        # Assuming transformer_output is of shape (batch_size, sequence_length, hidden_size)\n",
    "        final_step_activation = transformer_output[:, -1, :]\n",
    "\n",
    "        # Predict reward\n",
    "        reward_logits = self.reward_prediction_head(final_step_activation)\n",
    "        reward_probabilities = self.sigmoid(reward_logits)\n",
    "\n",
    "        return reward_probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 初始化模型和损失函数\n",
    "classifier_model =  GPT2RewardPredictor(config)\n",
    "# # Load the model\n",
    "trained_model_path = \"/data/shikangwei/gptrlt/model/gpt2sqlreward4\"\n",
    "checkpoint = torch.load(trained_model_path + \"/model_checkpoint.pth\")\n",
    "classifier_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "classifier_model.to(device='cuda:0')\n",
    "classifier_model.eval()  # 设置模型为评估模式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68274663-d107-4320-aa5e-fe3b3a8ea609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm_name = \"/data/shikangwei/gptrlt/givenmodel/fine_tune_model_sql_modsecurity\"  #论文给出训练好的模型\n",
    "lm_name = \"/data/shikangwei/gptrlt/givenmodel/pretrain_model_sql\"                #给出的预训练模型\n",
    "total_nums = int(64)\n",
    "txt_in_len=int(1)\n",
    "txt_out_len=int(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf595e-af7b-4e20-9046-c1bdf144aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference配置\n",
    "tokenizer = gpt2_tokenizer = GPT2Tokenizer.from_pretrained(lm_name)\n",
    "gpt2_model = GPT2HeadWithValueModel.from_pretrained(lm_name)\n",
    "\n",
    "_ = gpt2_model.eval()\n",
    "_ = gpt2_model.to(device)\n",
    "# 加载用于训练的模型（与推理模型可以是同一个模型的另一个实例）\n",
    "rl_name = \"/data/shikangwei/gptrlt/givenmodel/pretrain_model_sql\"  \n",
    "\n",
    "model_training = GPT2HeadWithValueModel.from_pretrained(rl_name)   #lm, 从开始开始训练\n",
    "model_training.train()  # 确保模型在训练模式，默认即为此模式\n",
    "# 检查是否有多个GPU可用，并使用DataParallel如果是的话\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "#     # 这里将模型包装在DataParallel中\n",
    "#     model_training = nn.DataParallel(model_training)\n",
    "model_training.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d6423-d845-4905-8596-778c99199a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "stopcount=0\n",
    "optimizer = optim.SGD(model_training.parameters(), lr=1.4e-5)\n",
    "kk=-1\n",
    "average=torch.tensor([], device=device)\n",
    "stopthreshold=0.01\n",
    "\n",
    "while True:\n",
    "    kk+=1\n",
    "    valueList, probList, responseList_original= generate(model_training, total_nums, txt_out_len)\n",
    "    cunAt = []\n",
    "    cunRt = []\n",
    "    cunT = []\n",
    "    cunrt = []\n",
    "    response1 = model_training(responseList_original[:][:])\n",
    "    xinde=response1[0]\n",
    "    pp=gpt2_model(responseList_original[:][:])\n",
    "    pp=pp[0]\n",
    "    for i in range(len(responseList_original)):\n",
    "        firsttry = responseList_original[i]\n",
    "    \n",
    "        T = txt_out_len-1\n",
    "        cunT.append(T)\n",
    "        \n",
    "        # 假设的Rt和V值\n",
    "        Rt = torch.zeros(T+1, device=device)  # T+1个值，从0到T\n",
    "        nested_tensor = firsttry.unsqueeze(0)\n",
    "        nested_tensor1 = []\n",
    "        nested_tensor1.append(nested_tensor)\n",
    "        \n",
    "        Rt[T] = classifier_model(nested_tensor1)[0]                                               #第六行 代码实现\n",
    "        \n",
    "        #计算0- T-1 区间的KL散度,直接保存到 \n",
    "        pprobs = F.softmax(pp[i], dim=1)\n",
    "        paiprobs = F.softmax(probList[i], dim=1) #probList的第i项\n",
    "        rtxinde = F.softmax(xinde[i], dim=1)\n",
    "        for ii in range(T):\n",
    "            # Rt[ii] = - 0.2* torch.log(paiprobs[ii][firsttry[ii+1]] /  pprobs[ii][firsttry[ii+1]])  #七八行 代码实现\n",
    "            Rt[ii] = 0\n",
    " \n",
    "        # Vt = valueList[i][:T+1]\n",
    " \n",
    "        # 图二中提到的gamma值\n",
    "        gamma = 0.99\n",
    "#         T = len(Rt) - 1  # Subtract 1 because tensors are 0-indexed\n",
    "#         # Initialize A_t to be the same shape as Rt, filled with zeros\n",
    "        A_t = torch.zeros_like(Rt)\n",
    "        Vt = valueList[i]\n",
    "        # Calculate A_t using the given formula\n",
    "        for t in range(T + 1):\n",
    "            discount = 1  # Discount factor γ^0 is 1\n",
    "            for k in range(t, T + 1):\n",
    "                A_t[t] += discount * Rt[k]\n",
    "                discount *= gamma  # Increase the discount factor γ by one power for each step\n",
    "            A_t[t] -= Vt[t]  # Subtract V(s_t) from the discounted sum of rewards\n",
    "        # # Calculate A_t using the one-step advantage estimate\n",
    "        # for t in range(T+1):\n",
    "        #     if(t<T):\n",
    "        #         A_t[t] = Rt[t] + gamma * valueList[i][t+1] - valueList[i][t]\n",
    "        #     else:\n",
    "        #         A_t[t] = Rt[t]   - valueList[i][t]\n",
    "            \n",
    "        # 计算返回估计\n",
    "        R_hat = torch.zeros_like(Rt)\n",
    "        for t in reversed(range(T+1)):\n",
    "            R_hat[t] = Rt[t] + (gamma * R_hat[t+1] if t < T else 0)                                #第九行代码实现\n",
    "\n",
    "        #rt保存\n",
    "        rt = torch.randn(T+1, device=device)  # T+1个值，从0到T\n",
    "        for ii in range(len(rt)):\n",
    "            rt[ii] = rtxinde[ii][firsttry[ii+1]] / paiprobs[ii][firsttry[ii+1]]\n",
    "  \n",
    "        #存储\n",
    "        cunAt.append(A_t) \n",
    "        cunRt.append(R_hat)\n",
    "        cunrt.append(rt)\n",
    "        \n",
    "    #计算policy value loss\n",
    "    policy_loss =torch.tensor(0.0, device= device)\n",
    "    epsilon = 0.2\n",
    "    for ii in range(len(cunT)):\n",
    "        for j in range(cunT[ii]+1):\n",
    "            clipped_ratio = torch.clamp(cunrt[ii][j], 1 - epsilon, 1 + epsilon)\n",
    "            policy_loss = policy_loss - torch.min(cunrt[ii][j] *cunAt[ii][j] , clipped_ratio * cunAt[ii][j])\n",
    "    policy_loss= policy_loss/len(cunT)\n",
    "    \n",
    "    #计算 value loss\n",
    "    value_loss = torch.tensor(0.0, device= device)\n",
    "    newvalueList= response1[2]\n",
    "    for i in range(len(cunT)):\n",
    "        for j in range(cunT[i]+1):\n",
    "            value_loss += (newvalueList[i][j] - cunRt[i][j])**2\n",
    "    value_loss = value_loss / len(cunT)\n",
    "    total_loss = policy_loss + value_loss\n",
    "    print(total_loss)\n",
    "    # 梯度下降步骤\n",
    "    optimizer.zero_grad()  # 清空之前的梯度\n",
    "    total_loss.backward()  # 反向传播，计算当前梯度\n",
    "    optimizer.step()       # 根据梯度更新模型参数\n",
    "\n",
    "    # 计算平均reward\n",
    "    averagereward =torch.tensor([0.0], device= device)\n",
    "    for i in range(len(cunT)):\n",
    "        averagereward[0] +=  cunRt[i][-1]\n",
    "    averagereward[0]= averagereward[0] / len(cunT)\n",
    "    \n",
    "    average = torch.cat((average, averagereward), 0)\n",
    "    average1= average.detach().cpu().numpy()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(average1, marker='o', linestyle='-', color='b')\n",
    "    plt.title('Average Reward per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "    if kk==1000 :\n",
    "        break\n",
    "#2024，3，20. 12，05。2GPU同时运行1000次，结束时间为：3,21. 19,00.. 31小时\n",
    "#2024, 3,22. 11.03. 2GPU同时运行1000次（NVlink）。结束时间： 3,23 15.49(看到) 不到29小时\n",
    "#2024，4，4 11.28 1GPU 1000次 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7490daa6-9e29-46a6-b4cd-d5fdd21e803a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575d8d9-9542-4a6a-9e19-63bd41850e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f4dee-3286-4268-a5a2-20e0a2dcd6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1fcbbe-bfe0-4802-9f81-12b4a95ad99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d08a7-5243-44be-9107-53dad191120a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ba76c-ed7b-420d-9863-1b2307c4a2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4031b-d12b-4320-97cb-7881791df0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73240b-5f6f-46a8-abd6-0a43bbc368ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f7a62-9960-43f7-9ab7-6072ff325043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f772e-a67f-4211-80de-729ebd916706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988cc03-ac3f-4ec7-ac87-c270330e4c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
